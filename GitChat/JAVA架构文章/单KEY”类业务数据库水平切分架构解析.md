## 内容提要

内容提要：

- 如果业务是允许手机号或者用户名或者邮箱登录，那在id中融入的方法是不是就不能用了？ 另外留多少位也是一个头疼问题，多了浪费，少了可能不够用。比如8库撑不住怎么办？
- 比如我在微信公众账号登陆用的微信隐式授权，会生成一个user_id 后续又下载app绑定了微信，这个时候用户有了两个账号就会涉及账号合并。不知道你们是怎么做的，用户的信息分表分库以后就会涉及账号合并后的数据迁移？不知道你们是怎么做数据迁移的？
- 实际的工作中，比较你按照用户去分库分表。前期规划的单库数据量是1个亿。单表承载1000万。后面卖家的订单会变大。就会超过预估的容量。一般你们的数据迁移方案是怎么做的？
- 之前有看到58开源了oceanus，不知道你们在线上是否有使用？你们现在有通过中间件做分库路由吗？
- 用户中心有第三方登入表如何设计，如何做到分库大数据时代主键如何考量？
- uid一旦生成 uid规则是随机数，下次重启应用会不会重复，数据量一大如何做到用户唯一性？
- 对于分库分表的问题，在没有历史包袱的前提下，是否能使用newsql数据库来解决这个问题，比如CockroachDB，TiDB这类天然支持水平扩展的数据库可以吗？
- 我看你们不允许 text，但是100亿的帖子肯定有超过varchar的最大长度。你们分多个字段存储吗？
- 分表后的字段查询是不是可以用一些搜索引擎来解决非索引字段的查询？
- 有一个在线实时的账户系统，用户转账，账户余额都是事实变的。又有一个新的账户系统，分库分表的，以前的单库oracle，这两个系统要同时跑，第一阶段，以单库oracle为主，以分库分表账户系统为辅，异步捞取或接收主库的交易事件，请问在这个阶段，2个系统都是近实时的，但是有时间差，怎样衡量新的账户系统交易受理都正确，交易受理后各个账户余额都相等，没有差错？
- 对于帖子中包含隐藏html标签导致数据长度超过限制的如何处理？
- 目前大部分的磁盘，在qd低的情况下iops都不是很理想，目前58自建的存储系统有考虑过通过并发扫描的方式来充分发挥iops在单次查询中的性能吗？
- 目前困扰我的地方是余额都实时变的，不好比对余额是否正确，因为老的系统里是没有处理完某个交易后的余额信息的，请问该怎么办？
- 现在的问题是验证阶段，如果验证2个系统的正确性，余额是否相等？

------

**问：如果业务是允许手机号或者用户名或者邮箱登录，那在id中融入的方法是不是就不能用了？ 另外留多少位也是一个头疼问题，多了浪费，少了可能不够用。比如8库撑不住怎么办？**

**答：** uid中打入`login_name` 基因，email/phone则无法使用“基因法”定位库。一般来说，会使用一个不变的string来生成基因，例如`login_name`，email/phone与uid的绑定关系如果会变的话，不能用来生成基因。

“基因法”需要提前做容量预估，例如8个bit是比较合适的。此时email/phone的查询，可以使用“索引表法”或者“缓存映射法”来定位库，即文中的头两种方法。

------

**问：比如我在微信公众账号登陆用的微信隐式授权，会生成一个user_id 后续又下载app绑定了微信，这个时候用户有了两个账号就会涉及账号合并。不知道你们是怎么做的，用户的信息分表分库以后就会涉及账号合并后的数据迁移？不知道你们是怎么做数据迁移的？**

**答：** email/phone/login_name都可以登录，但实际上是一个uid。如果是两个uid，可以认为是两个用户。

帐号合并不好搞，设计很多业务，好友是否要合并？余额怎么合并？订单是否要合并？比较复杂。

如果确实是一个用户，email/phone/login_name可以对应到一个uid。

------

**问：实际的工作中，比较你按照用户去分库分表。前期规划的单库数据量是1个亿。单表承载1000万。后面卖家的订单会变大。就会超过预估的容量。一般你们的数据迁移方案是怎么做的？**

**答：**这是一个很好的问题，涉及到容量变化。

常用的方案有两种：

秒级扩容的方案是一种[《数据库秒级平滑扩容架构方案》](http://mp.weixin.qq.com/s/BLOneOs-cPxP_9b5eH8oQA)。

[《100亿数据平滑数据迁移，不影响服务》](http://mp.weixin.qq.com/s/Ozqu2A7Sy_TGKkF6yF1rDQ)，平滑迁移这里又有两种方案，这几种方案都很精妙，建议大家细看一下。

------

**问：之前有看到58开源了oceanus，不知道你们在线上是否有使用？你们现在有通过中间件做分库路由吗？**

**答：** 58数据库中间件，线上在用，中间件路由，自研的中间件。

------

**问：你说过通过usercenter去找分库分表信息 ，那这里的瓶颈就在usercenter 。所有用户信息都在这个表，是个超级大表。分多个表看你有说，但是要读多次有没有办法解决？**

**答：**分为以下几个方面：

1. user-service不是瓶颈，服务层可以水平扩展。
2. 实际应该使用分库，而不是分表。
3. 不存在“大表”一说，拆分后数据可以水平扩展。

曾经，58同城的100亿帖子都存在数据库里,曾经mysql，现在存自研的一个存储系统：[《58到家数据库30条军规解读》](http://mp.weixin.qq.com/s/Yjh_fPgrjuhhOZyVtRQ-SA)，[《100亿数据1万属性数据架构设计》](http://mp.weixin.qq.com/s/3O3kPSwV-tAeYdy2ZRACpg)。

------

**问：用户中心有第三方登入表如何设计，如何做到分库大数据时代主键如何考量？**

**答：** User(uid, login_name, phone, email, sex, age)，如果有第三方登录，一定有类似第三方帐号到uid的映射关系，用户中心仍然是uid分库，映射表是第三方帐号分库（类似于文章中方案一/方案二的映射表）。

------

**问：uid一旦生成 uid规则是随机数，下次重启应用会不会重复，数据量一大如何做到用户唯一性？**

**答：**这是一个分布式id生成的问题：[《细聊分布式ID生成方法》](http://mp.weixin.qq.com/s/0H-GEXlFnM1z-THI8ZGV2Q)。

------

**问：对于分库分表的问题，在没有历史包袱的前提下，是否能使用newsql数据库来解决这个问题，比如CockroachDB，TiDB这类天然支持水平扩展的数据库可以吗？**

**答：**这两个数据库我不是很了解，不知道线上是否有几十亿数据量，几万吞吐量的应用，mongoDB天然支持auto-sharding，我的经验是不靠谱，自己拆分心里有底。

对于新技术，我的观点是：需要学习，研究，但应用到线上一定要慎重，研发、测试、运维、遇到问题解决不了，各种成本。

------

**问：我看你们不允许 text，但是100亿的帖子肯定有超过varchar的最大长度。你们分多个字段存储吗？**

**答：** 这个是58到家的mysql军规，58同城原来帖子内容印象中mysql是用text（或者varchar）存储的，限定了最大长度之后，严格执行。

------

**问：分表后的字段查询是不是可以用一些搜索引擎来解决非索引字段的查询？**

**答：** 看业务场景，文章中描述了两类业务，一类就是用外置索引（搜索引擎）来解决的。

------

**问：有一个在线实时的账户系统，用户转账，账户余额都是事实变的。又有一个新的账户系统，分库分表的，以前的单库oracle，这两个系统要同时跑，第一阶段，以单库oracle为主，以分库分表账户系统为辅，异步捞取或接收主库的交易事件，请问在这个阶段，2个系统都是近实时的，但是有时间差，怎样衡量新的账户系统交易受理都正确，交易受理后各个账户余额都相等，没有差错？**

**答：**这是一个很好的问题，涉及双系统，平滑迁移，数据同步。

首先，一定同一个时间只有一个系统对线上提供服务，另一个系统standby，确认数据一致后，会切换。问题转化为，如何不停服务的平滑迁移。

[《100亿数据平滑数据迁移,不影响服务》](http://mp.weixin.qq.com/s/Ozqu2A7Sy_TGKkF6yF1rDQ)，这篇文章的两个方案应该能给你一些启示，如果同时提供服务，确实难以保证数据完全一致。

------

**问：对于帖子中包含隐藏html标签导致数据长度超过限制的如何处理？**

**答：**存储端不管存的是text，还是html，还是json。超过长度就不行。

------

**问：目前大部分的磁盘，在qd低的情况下iops都不是很理想，目前58自建的存储系统有考虑过通过并发扫描的方式来充分发挥iops在单次查询中的性能吗？**

**答：**架构设计，扩展性很重要，不能依赖于单机性能，数据库无论如何要考虑水平扩展、这样可以通过加机器就扩展性能。

------

**问：目前困扰我的地方是余额都实时变的，不好比对余额是否正确，因为老的系统里是没有处理完某个交易后的余额信息的，请问该怎么办？**

**答：**你们是怎么同步数据的，是双写吗？特别是多条数据返回，别用字符串索引（当然，login_name这样的迫不得已，幸好是单条数据返回，并且字符串不长）。

------

**问：现在的问题是验证阶段，如果验证2个系统的正确性，余额是否相等？**

**答：**数据一直在变，某一个时刻，很可能有很少的数据差异，确实会这样（特别是吞吐量比较大的时候）。

1. 数据同步有时延.
2. MQ消息能保证可达吗。
3. 异步双写，本质是分布式事务，不能保证两边都成功的。

写oracle，异步写另一个数据库不能同时保证都成功。很可能出现，写oracle成功，异步写失败，就不一致了。数据迁移的文章里有提到，得有检测不一致的机制。



## 文章实录



本文将以“用户中心”为例，介绍**“单KEY”类业务**，随着数据量的逐步增大，数据库性能显著降低，数据库水平切分相关的架构实践：

- 如何来实施水平切分。
- 水平切分后常见的问题 。
- 典型问题的优化思路及实践。

### 一、用户中心

**用户中心**是一个非常常见的业务，主要提供用户注册、登录、信息查询与修改的服务，其核心元数据为：

User(uid, login_name, passwd, sex, age, nickname, …)

其中：

- uid为用户ID，主键。
- login_name, passwd, sex, age, nickname, …等用户属性。

数据库设计上，一般来说在业务初期，单库单表就能够搞定这个需求，典型的架构设计为：

![enter image description here](http://images.gitbook.cn/ff396150-4398-11e7-9f38-3b023be5d43b)

- user-center：用户中心服务，对调用者提供友好的RPC接口。
- user-db：对用户进行数据存储。

### 二、用户中心水平切分方法

当数据量越来越大时，需要多用户中心进行水平切分，常见的水平切分算法有“**范围法**”和“**哈希法**”。

#### 范围法

范围法：以用户中心的业务主键uid为划分依据，将数据水平切分到两个数据库实例上去：

![enter image description here](http://images.gitbook.cn/08f41280-4399-11e7-baef-9101ce25f77b)

- user-db1：存储0到1千万的uid数据。
- user-db2：存储0到2千万的uid数据。

范围法的**优点**是：

- 切分策略简单，根据uid，按照范围，user- center很快能够定位到数据在哪个库上。
- 扩容简单，如果容量不够，只要增加user-db3即可。

范围法的**不足**是：

- uid必须要满足递增的特性。
- 数据量不均，新增的user-db3，在初期的数据会比较少。
- 请求量不均，一般来说，新注册的用户活跃度会比较高，故user-db2往往会比user-db1负载要高，导致服务器利用率不平衡。

#### 哈希法

哈希法：也是以用户中心的业务主键uid为划分依据，将数据水平切分到两个数据库实例上去：

![enter image description here](http://images.gitbook.cn/0f4273c0-4399-11e7-84fa-2d89241c4e36)

- user-db1：存储uid取模得1的uid数据。
- user-db2：存储uid取模得0的uid数据。

哈希法的**优点**是：

- 切分策略简单，根据uid，按照hash，user-center很快能够定位到数据在哪个库上。
- 数据量均衡，只要uid是均匀的，数据在各个库上的分布一定是均衡的。
- 请求量均衡，只要uid是均匀的，负载在各个库上的分布一定是均衡的。

哈希法的**不足**是：

- 扩容麻烦，如果容量不够，要增加一个库，重新hash可能会导致数据迁移，如何平滑的进行数据迁移，是一个需要解决的问题

### 三、用户中心水平切分后带来的问题

使用uid来进行水平切分之后，整个用户中心的业务访问会遇到什么问题呢？

对于uid属性上的查询可以直接路由到库，假设访问uid=124的数据，取模后能够直接定位db-user1：

![enter image description here](http://images.gitbook.cn/1cdea4e0-4399-11e7-84fa-2d89241c4e36)

对于非uid属性上的查询，例如login_name属性上的查询，就悲剧了：

![enter image description here](http://images.gitbook.cn/26c261e0-4399-11e7-8f9a-571baae97adf)

假设访问login_name=shenjian的数据，由于不知道数据落在哪个库上，往往需要遍历所有库，当分库数量很多时，性能会显著降低。

如何解决分库后，非uid属性上得查询问题，是后文要重点讨论的内容。

### 四、用户中心非uid属性查询需求分析

**任何脱离业务的架构设计都是耍流氓**，在进行架构讨论之前，先来对业务进行简要分析，看非uid属性上有哪些查询需求。

根据楼主这些年的架构经验，用户中心非uid属性上经常有两类业务需求：

- 用户侧，前台访问，最典型的有两类需求。

- **用户登录**：通过login_name/phone/email查询用户的实体，1%请求属于这种类型

- **用户信息查询**：登录之后，通过uid来查询用户的实例，99%请求属这种类型

  用户侧的查询基本上是单条记录的查询，访问量较大，服务需要高可用，并且对一致性的要求较高。

- 运营侧，后台访问，根据产品、运营需求，访问模式各异。

  按照年龄、性别、头像、登陆时间、注册时间来进行查询。

  运营侧的查询基本上是批量分页的查询，由于是内部系统，访问量很低，对可用性的要求不高，对一致性的要求也没这么严格。

这两类不同的业务需求，应该使用什么样的架构方案来解决呢？

### 五、用户中心水平切分架构思路

用户中心在数据量较大的情况下，使用uid进行水平切分，对于非uid属性上的查询需求，架构设计的核心思路为：

- 针对用户侧，应该采用“**建立非uid属性到uid的映射关系**”的架构方案。
- 针对运营侧，应该采用“**前台与后台分离**”的架构方案。

### 六、用户中心-用户侧最佳实践

#### 索引表法

**思路**：uid能直接定位到库，login*name不能直接定位到库，如果通过login*name能查询到uid，问题解决。

**解决方案**：

- 建立一个索引表记录login_name->uid的映射关系。
- 用login_name来访问时，先通过索引表查询到uid，再定位相应的库。
- 索引表属性较少，可以容纳非常多数据，一般不需要分库。
- 如果数据量过大，可以通过login_name来分库。

**潜在不足**：多一次数据库查询，性能下降一倍。

#### 缓存映射法

**思路**：访问索引表性能较低，把映射关系放在缓存里性能更佳。

**解决方案**：

- login_name查询先到cache中查询uid，再根据uid定位数据库。
- 假设cache miss，扫描全库获取login_name对应的uid，放入cache.
- login_name到uid的映射关系不会变化，映射关系一旦放入缓存，不会更改，无需淘汰，缓存命中率超高。
- 如果数据量过大，可以通过login_name进行cache水平切分。

**潜在不足**：多一次cache查询。

#### login_name生成uid

**思路**：不进行远程查询，由login_name直接得到uid.

**解决方案**：

- 在用户注册时，设计函数login*name生成uid，uid=f(login*name)，按uid分库插入数据。
- 用login*name来访问时，先通过函数计算出uid，即uid=f(login*name)再来一遍，由uid路由到对应库。

**潜在不足**：该函数设计需要非常讲究技巧，有uid生成冲突风险。

#### login_name基因融入uid

**思路**：不能用login*name生成uid，可以从login*name抽取“基因”，融入uid中：

![enter image description here](http://images.gitbook.cn/3cd10cc0-4399-11e7-9f38-3b023be5d43b)

假设分8库，采用uid%8路由，潜台词是，uid的最后3个bit决定这条数据落在哪个库上，这3个bit就是所谓的“基因”。

**解决方案**：

- 在用户注册时，设计函数login*name生成3bit基因，login*name*gene=f(login*name)，如上图粉色部分。
- 同时，生成61bit的全局唯一id，作为用户的标识，如上图绿色部分。
- 接着把3bit的login*name*gene也作为uid的一部分，如上图屎黄色部分。
- 生成64bit的uid，由id和login*name*gene拼装而成，并按照uid分库插入数据。
- 用login*name来访问时，先通过函数由login*name再次复原3bit基因。
- login*name*gene=f(login*name)，通过login*name_gene%8直接定位到库。

### 七、用户中心-运营侧最佳实践

前台用户侧，业务需求基本都是单行记录的访问，只要建立非uid属性login_name/phone/email到uid的映射关系，就能解决问题。

后台运营侧，业务需求各异，基本是批量分页的访问，这类访问计算量较大，返回数据量较大，比较消耗数据库性能。

如果此时前台业务和后台业务公用一批服务和一个数据库，有可能导致，由于后台的“少数几个请求”的“批量查询”的“低效”访问，导致数据库的cpu偶尔瞬时100%，影响前台正常用户的访问（例如，登录超时）。

![enter image description here](http://images.gitbook.cn/50647e70-4399-11e7-8f9a-571baae97adf)

而且，为了满足后台业务各类“奇形怪状”的需求，往往会在数据库上建立各种索引，这些索引占用大量内存，会使得用户侧前台业务uid/login_name上的查询性能与写入性能大幅度降低，处理时间增长。

对于这一类业务，应该采用“前台与后台分离”的架构方案：

![enter image description here](http://images.gitbook.cn/5aef9f00-4399-11e7-8f9a-571baae97adf)

用户侧前台业务需求架构依然不变，产品运营侧后台业务需求则抽取独立的web/service/db来支持，解除系统之间的耦合，对于“业务复杂”“并发量低”“无需高可用”“能接受一定延时”的后台业务：

- 可以去掉service层，在运营后台web层通过dao直接访问db.
- 不需要反向代理，不需要集群冗余。
- 不需要访问实时库，可以通过MQ或者线下异步同步数据。
- 在数据库非常大的情况下，可以使用更契合大量数据允许接受更高延时的“索引外置”或者“HIVE”的设计方案。

![enter image description here](http://images.gitbook.cn/68a41310-4399-11e7-baef-9101ce25f77b)

### 八、总结

本文以“用户中心”为典型的 “单KEY”类业务，水平切分的架构点，做了这样一些介绍。

水平切分方式：

- 范围法
- 哈希法

水平切分后碰到的问题：

- 通过uid属性查询能直接定位到库，通过非uid属性查询不能定位到库。

非uid属性查询的典型业务：

- 用户侧，前台访问，单条记录的查询，访问量较大，服务需要高可用，并且对一致性的要求较高。
- 运营侧，后台访问，根据产品、运营需求，访问模式各异，基本上是批量分页的查询，由于是内部系统，访问量很低，对可用性的要求不高，对一致性的要求也没这么严格。

这两类业务的架构设计思路：

- 针对用户侧，应该采用“建立非uid属性到uid的映射关系”的架构方案。
- 针对运营侧，应该采用“前台与后台分离”的架构方案。

用户前台侧，“建立非uid属性到uid的映射关系”最佳实践：

- 索引表法：数据库中记录login_name->uid的映射关系。
- 缓存映射法：缓存中记录login_name->uid的映射关系。
- login_name生成uid.
- login_name基因融入uid.

运营后台侧，“前台与后台分离”最佳实践：

- 前台、后台系统web/service/db分离解耦，避免后台低效查询引发前台查询抖动。
- 可以采用数据冗余的设计方式。
- 可以采用“外置索引”（例如ES搜索系统）或者“大数据处理”（例如HIVE）来满足后台变态的查询需求。
- ​

