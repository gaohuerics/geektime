## 内容提要

内容提要：

- 有一类典型操作是列表页操作，这个过程是怎样的？对分库分表策略有什么影响？
- 帖子用心有帖子打分，搜索加权，广告推广付费。这些都会影响排序，请问你们怎么处理打分后分页排序问题？
- 我最近一直看到元数据，元数据到底是什么意思，是数据库的基础数据？还是指的是注解编程模式？
- 数据库要分片前期的准备工作是什么，怎么区分这表要分片？如果这个表还能数据继续sql优化不是更好。 如何选择分区键，难道是就采用本文中的UID建立索引来选择吗？如何分配分片中的数据？有什么好的方案？
- 文章中的库对于不同的数据库软件分别是指什么？
- 对于基因分库，如果用户分布足够均匀，但是用户的发帖量不均衡，或者用户的活跃程度差异，有没有什么好的解决思路？
- 分布式事务如何处理？
- 分库分表在微服务模式下怎么配合使用，比如用户部门之类我们做成一个服务，这时候订单要取部门信息，以前是数据库关联查询，现在都分开了，怎么查询才有效率？
- 为满足业务，需要多个表进行连接， 例如：分类、内容、发布人、留言等 ，每个表数据都在几十W以上，数据库配置不高的情况下，如何设计提高性能？

------

**问：有一类典型操作是列表页操作，这个过程是怎样？对分库分表策略有什么影响？**

**答：**单库的时候，列表分页可以用order by ... offset ... limit ...实现分库分表之后，列表分页的需求该如何实现？

[《互联网数据库“跨库分页”架构技术实践》](http://gitbook.cn/m/mazi/article/5896c2e7f2b669527d7a7c8a?isLogArticle=no&readArticle=yes&sut=85a2c6005feb11e7a71e934b5d1d097b)，我在2月份GitChat的一篇文章专门深入的探讨了这个问题，有兴趣的可以看一下。

------

**问：帖子用心有帖子打分，搜索加权，广告推广付费。这些都会影响排序，请问你们怎么处理打分后分页排序问题？**

**答：**帖子库的设计，uid/tid用一些技巧，简历索引后，可以满足uid/tid上的查询需求，那其他字段上的查询需求如何满足？ 如果是后台字段的查询需求，可以使用[《从用户中心开始，聊“单KEY”类业务数据库水平切分架构实践》](http://gitbook.cn/m/mazi/activity/58ecc473122a61e241e04c5c?giftToken=93511021-414d-11e7-b65e-373266a6c8a1&sut=2e7ffdb05fec11e7ac55dbb7748c39e3)中提到的“前台与后台分离”的架构设计方法解决。

如果是像搜索加权这样的业务需求，可以独立一个搜索系统来满足相关查询需求，帖子服务与帖子搜索服务之间通过消息来同步信息的修改。

帖子服务满足线上uid/tid的查询需求。帖子搜索服务来满足就倒排，打分，排序的查询需求。

[《100亿数据1万属性数据架构设计》](http://mp.weixin.qq.com/s/3O3kPSwV-tAeYdy2ZRACpg)，这篇文章，讲述58同城帖子中心的架构设计，数据量、并发量、扩展性等有更加详细的讲述。

------

**问：我最近一直看到元数据，元数据到底是什么意思？是数据库的基础数据？还是指的是注解编程模式？**

**答：**前者，元数据是指存在数据库里的基础数据，与之对应的，可能有存在搜索系统里的索引数据。元数据管理服务，可以通过消息，来通知索引服务，元数据发生了修改，索引数据也请进行相应的修改。

------

**问：数据库要分片前期的准备工作是什么，怎么区分这表要分片？如果这个表还能数据继续sql优化不是更好。 如何选择分区键，难道是就采用本文中的UID建立索引来选择吗？如何分配分片中的数据？有什么好的方案？**

**答：**分片解决的是“数据量大”这个问题的，和SQL优化是两个问题。SQL低质量，理所当然应该优化。数据量10亿，一个命中索引的SQL也是很慢的，必须通过分片来降低数据量。

“如何选择分区键，难道是就采用本文中的UID建立索引来选择吗？”

有四类业务场景：单KEY、一对多、多对多、多KEY。

每种场景的处理方式不一样，分别对应本专题包的四篇文章。

一对多的业务场景，就如帖子，就是使用uid分库，tid打入基因的方式。

“如何分配分片中的数据，有什么好的方案？”，范围，哈希是两种常见的方案。[《前言：数据库典型架构实践》](http://gitbook.cn/m/mazi/activity/58ecc45a54f0e5f04170425d?sut=66fca2505fed11e7a71e934b5d1d097b)，之前在GitChat上发布的这篇文章，有更系统的讲解。

------

**问：文章中的库对于不同的数据库软件分别是指什么？**

**答：** 文章讲述的是一种架构设计思想，和具体使用哪种数据库软件无关，ms-sql，mysql，ORCLE什么的都没有关系。

------

**问：对于基因分库，如果用户分布足够均匀，但是用户的发帖量不均衡，或者用户的活跃程度差异，有没有什么好的解决思路？**

**答：**只要uid是均衡的，每个用户发布的平均帖子数是均衡的，每个库的数据就是均衡的。即使用户发帖数是不均匀的，用户活跃度是有差异的，只要uid是均匀的，数据最终分布也是均匀的。

再举例细一点。

假设有“一些”用户的发帖量是普通用户的100倍，或者这些用户的活跃度是普通用户的100倍。这“一些”用户肯定不止一个，假设有1000个，只要这些用户的uid是均匀的，假设分4个库，4个库上分别有250个左右的用户，整体的数据也是均匀的。

------

**问：分布式事务如何处理？**

**答：**这个问题太泛了，最好有具体的场景。我只能这么说：

1. 分布式事务，特别是高吞吐量下想要保证分布式事务的一致性，是业界未解决的难题。
2. 方法论上，解决方向是“不能做到完全一致性，只能做到尽快的发现不一致并修复不一致”。

如果有兴趣，未来我举几个具体的例子，和大家展开讲解。

------

**问：分库分表在微服务模式下怎么配合使用，比如用户部门之类我们做成一个服务，这时候订单要取部门信息，以前是数据库关联查询，现在都分开了，怎么查询才有效率？**

**答：**服务化要做到数据库私有化，只有服务可以访问对应的数据库，任何上游不能绕过服务调用数据库。对于JOIN，高吞吐的情况下，JOIN对于数据库性能影响较大，一般禁止使用，需要转化为两次服务调用来实现。

------

**问：为满足业务，需要多个表进行连接， 例如：分类、内容、发布人、留言等 ，每个表数据都在几十W以上，数据库配置不高的情况下，如何设计提高性能？**

**答：**所有的JOIN，只有在数据量不大的情况下可以使用。大吞吐量，禁止使用JOIN，转化为其他两次查询。一次调用订单服务取订单信息，一次调用部门服务取部门信息，为何一定要JOIN？58同城100亿帖子，10万并发，业务也很复杂，从没有使用过JOIN。



## 文章实录



查看本场Chat

本文将以“帖子中心”为例，介绍“1对多”类业务，随着数据量的逐步增大，数据库性能显著降低，数据库水平切分相关的架构实践：

- 如何来实施水平切分
- 水平切分后常见的问题
- 典型问题的优化思路及实践

### 一、什么是1对多关系

所谓的“1对1”“1对多”“多对多”，来自数据库设计中的“实体-关系”ER模型，用来描述实体时间的映射关系：

**1对1**

一个用户只有一个登录名，一个uid对应一个login_name，这是一个1对1的关系。

**1对多**

一个用户可以发多条微博，一条微博只有一个发送者；

一个uid对应多个msg*id，一个msg*id只对应一个uid；

这是一个1对多的关系。

**多对多**

一个用户可以关注多个用户，一个用户也可以被多个粉丝关注，这是一个多对多的关系。

### 二、帖子中心业务分析

![enter image description here](http://images.gitbook.cn/d640e9b0-597a-11e7-a126-29802fa6a6a6)

帖子中心是一个典型的1对多业务。

![enter image description here](http://images.gitbook.cn/df0ea2d0-597a-11e7-a74c-45c8a6f722fd)

一个用户可以发布多个帖子，一个帖子只对应一个发布者。

**任何脱离业务的架构设计都是耍流氓**，先来看看帖子中心对应的业务需求。

帖子中心，是一个提供帖子发布，修改，删除，查看，搜索的服务。

**写操作需求**

- 发布(insert)帖子
- 修改(update)帖子
- 删除(delete)帖子

**读操作需求**

- 通过tid查询帖子实体，单行查询。
- 通过uid查询用户发布过的帖子，列表查询。
- 帖子检索，例如通过时间、标题、内容搜索符合条件的帖子。

在数据量较大，并发量较大的时候，通常通过**元数据与索引数据分离**的架构来满足不同类型的需求：

![enter image description here](http://images.gitbook.cn/ee351d70-597a-11e7-a126-29802fa6a6a6)

架构中的几个关键点：

- tiezi-center服务。
- tiezi-db：提供元数据存储。
- tiezi-search搜索服务。
- tiezi-index：提供索引数据存储。
- MQ：tiezi-center与tiezi-search通讯媒介，一般不直接使用RPC调用，而是通过MQ对两个子系统解耦。

其中，tiezi-center和tiezi-search分别满足两类不同的读需求：

![enter image description here](http://images.gitbook.cn/f8bf7ab0-597a-11e7-a126-29802fa6a6a6)

如上图所示：

- tid和uid上的查询需求，可以由tiezi-center从元数据读取并返回。
- 其他类检索需求，可以由tiezi-search从索引数据检索并返回。

对于写操作：

![enter image description here](http://images.gitbook.cn/ff9bc410-597a-11e7-a74c-45c8a6f722fd)

如上图所示：

- 增加，修改，删除的操作都会从tiezi-center发起。
- tiezi-center修改元数据。
- tiezi-center将信息修改通知发送给MQ。
- tiezi-search从MQ接受修改信息。
- tiezi-search修改索引数据。

tiezi-search可以使用Solr，ES等开源架构实现，数据量/并发量达到10亿/10万级别时可以自研搜索引擎，这一块不是本文的重点，后文将重点描述帖子中心元数据这一块的水平切分设计。

### 三、帖子中心元数据设计

通过帖子中心业务分析，很容易了解到，其核心元数据为：

Tiezi(tid, uid, time, title, content, …)

其中：

- tid为帖子ID，主键。
- uid为用户ID，发帖人。
- time, title, content …等为帖子属性。

数据库设计上，在业务初期，单库就能满足元数据存储要求，其典型的架构设计为：

![enter image description here](http://images.gitbook.cn/103556b0-597b-11e7-a74c-45c8a6f722fd)

- tiezi-center：帖子中心服务，对调用者提供友好的RPC接口。
- tiezi-db：对帖子数据进行存储。

在相关字段上建立索引，就能满足相关业务需求：

- 帖子记录查询，**通过tid查询，约占读请求量的90%**

> select * from t_tiezi where tid=$tid

- 帖子列表查询，**通过uid查询其发布的所有帖子，约占读请求量的10%**

> select * from t_tiezi where uid=$uid

### 四、帖子中心水平切分-tid切分法

当数据量越来越大时，需要对帖子数据的存储进行线性扩展，既然是帖子中心，并且帖子记录查询量占了总请求的90%，很容易想到通过tid字段取模来进行水平切分：

![enter image description here](http://images.gitbook.cn/1a5f6b80-597b-11e7-a126-29802fa6a6a6)

这个方法简单直接，优点是：

- 100%写请求可以直接定位到库。
- 90%的读请求可以直接定位到库。

缺点是：

- 一个用户发布的所有帖子可能会落到不同的库上，10%的请求通过uid来查询会比较麻烦。

![enter image description here](http://images.gitbook.cn/21fd3020-597b-11e7-a74c-45c8a6f722fd)

如上图，一个uid访问需要遍历所有库。

### 五、帖子中心水平切分-uid切分法

**有没有一种切分方法，确保同一个用户发布的所有帖子都落在同一个库上**，而在查询一个用户发布的所有帖子时，不需要去遍历所有的库呢？

解答：使用uid来分库可以解决这个问题。

新出现的问题：如果使用uid来分库，确保了一个用户的帖子数据落在同一个库上，**那通过tid来查询，就不知道这个帖子落在哪个库上了**，岂不是还需要遍历全库，需要怎么优化呢？

解答：tid的查询是单行记录查询，只要在数据库（或者缓存）新增索引记录，新增tid到uid的映射关系，就能解决这个问题。

新增一个索引库：t_mapping(tid, uid)

- 这个库只有两列，可以承载很多数据。
- 即使数据量过大，索引库可以利用tid水平切分。
- 这类kv形式的索引结构，可以很好的利用cache优化查询性能。
- 一旦帖子发布，tid和uid的映射关系就不会发生变化，cache的命中率会非常高。

使用uid分库，并增加索引库记录tid到uid的映射关系之后，每当有uid上的查询：

![enter image description here](http://images.gitbook.cn/33f170c0-597b-11e7-a126-29802fa6a6a6)

可以通过uid直接定位到库。

每当有tid上的查询：

![enter image description here](http://images.gitbook.cn/3bf32750-597b-11e7-a60d-63bbdd8fa1b3)

- 先查询索引表，通过tid查询到对应的uid.
- 再通过uid定位到库。

这个方法的优点是：

- 一个用户发布的所以帖子落在同一个库上。
- 10%的请求过过uid来查询列表，可以直接定位到库。
- 索引表cache命中率非常高，因为tid与uid的映射关系不会变。

缺点是：

- 90%的tid请求，以及100%的修改请求，不能直接定位到库，需要先进行一次索引表的查询，当然这个查询非常非常块，通常在5ms内可以返回。
- 数据插入时需要查询元数据与索引表，可能引发潜在的一致性问题。

### 六、帖子中心水平切分-基因法

**有没有一种方法，既能够通过uid定位到库，又不需要建立索引表来进行二次查询呢**，这就是本文要叙述的“1对多”业务分库最佳实践，**基因法**。

**什么是分库基因？**

通过uid分库，假设分为16个库，采用uid%16的方式来进行数据库路由，这里的uid%16，其本质是**uid的最后4个bit决定这行数据落在哪个库上**，这4个bit，就是分库基因。

**什么是基因法分库？**

在“1对多”的业务场景，使用“1”分库，在“多”的数据id生成时，id末端加入分库基因，就能同时满足“1”和“多”的分库查询需求。

![enter image description here](http://images.gitbook.cn/4ab62530-597b-11e7-aa1d-ef37a6bf11ee)

如上图所示，uid=666的用户发布了一条帖子：

- 使用uid%16分库，决定这行数据要插入到哪个库中。
- 分库基因是uid的最后4个bit，即1010。
- 在生成tid时，先使用一种分布式ID生成算法生成前60bit（上图中绿色部分）。
- 将分库基因加入到tid的最后4个bit（上图中粉色部分），拼装成最终的64bit帖子tid（上图中蓝色部分）。

通过这种方法保证，同一个用户发布的所有帖子的tid，都落在同一个库上，tid的最后4个bit都相同，于是：

- 通过uid%16能够定位到库。
- 通过tid%16也能定位到库。

完美！

潜在问题一：**同一个uid发布的tid落在同一个库上，会不会出现数据不均衡？**

回答：只要uid是均衡的，每个用户发布的平均帖子数是均衡的，每个库的数据就是均衡的。

潜在问题二：**最开始分16库，分库基因是4bit，未来要扩充成32库，分库基因变成了5bit，那怎么办？**

回答：需要提前做好容量预估，例如事先规划好5年内数据增长256库足够，就提前预留8bit基因。

### 七、总结

将以“帖子中心”为典型的 “1对多”类业务，在架构上，采用元数据与索引数据分离的设计方法：

- 帖子服务，元数据满足uid和tid的查询需求。
- 搜索服务，索引数据满足复杂搜索寻求。

对于元数据的存储，在数据量较大的情况下，有三种常见的切分方法：

- tid切分法，按照tid分库，同一个用户发布的帖子落在不同的库上，通过uid来查询要遍历所有库。
- uid切分法，按照uid分库，同一个用户发布的帖子落在同一个库上，需要通过索引表或者缓存来记录tid与uid的映射关系，通过tid来查询时，先查到uid，再通过uid定位库。
- 基因法，按照uid分库，在生成tid里加入uid上的分库基因，保证通过uid和tid都能直接定位到库。