**问：1）不停机升级，如果用了自增主键，在建新库的时候，起始ID需设置为旧库最大值+1。这个时候得锁表吗？就是想保证部分未升级的旧服务节点不能写入新数据，导致新旧库主键不同步。 对于这样的问题有什么好建议吗？还是对于自增主键是否不建议使用呢？2）外键，也是个麻烦事，因为新库现在还没有导老数据过来。 有什么好的建议吗？取消外键后同步，还是先迁移部分外键相关数据，这又会涉及如何追平的问题了。**

**答：** 1）我们对自增主键的玩法，即使自增主键，这个主键也必须是无意义的，不允许业务上有含义，不允许上游保存这个自增ID，这样导表不会有影响。建议大伙的SQL军规，也加上对自增主键类似的约定。

**答：** 2）外键，我们的玩法是，禁止使用外键，业务层/服务层 去做检查

------

**问：两个方案都不错，双写写新库的时候是同步写还是交给Mq之类的异步写？**

**答：**双写新库，同步双写，异步双写都可以，各自的优缺点是：

1. 同步双写简单，一致性更强，但时延会加倍。
2. 异步双写要引入MQ组件，流程更长更复杂，但时延不会有影响。

------

**问：双写用过。主库损坏飘从库，对应用透明有没好方案？**

**答：**这是一个写库高可用问题，写库挂了漂移从库，但如果从库是读库，需要人工接入，改变数据库集群结构。可以使用这样的方案避免人工介入：

![enter image description here](http://images.gitbook.cn/a62883f0-0efc-11e7-a222-b594197bdbce)

以mysql为例，可以设置两个mysql双主同步，一台对线上提供服务，另一台冗余以保证高可用，常见的实践是keepalived存活探测，相同virtual IP提供服务。

![enter image description here](http://images.gitbook.cn/c056f040-0efc-11e7-a222-b594197bdbce)

当写库挂了的时候，keepalived能够探测到，会自动的进行故障转移，将流量自动迁移到shadow-db-master，由于使用的是相同的virtual IP，这个切换过程对调用方是透明的。

------

**问：双写以前一直听说，但没有自己操作过。我有2个问题。1）如果开启双写，个别记录写新库失败，但旧库成功，这种情况靠比对工具发现吗？ 2）比对工具实现逻辑需要注意那些细节？如按照什么顺序（如N秒前更新时间升序）捞数据比对？比对新老库每条记录的所有列吗？**

**答：** 1）可以靠比对工具发现；2）不对数据注意几个点：

- 按照一个属性，一般是primary key从min到max，分批限速比对。
- 新库要比对旧库，旧库也要比对新库，双向的，以免出现一边库的数据量多于另一边，漏掉这种情况。

------

**问：双写需要保证事务一致性吗，写新库失败，写旧库成功还给用户返回正确结果吗？**

**答：** 分布式事务很难保证一致性。需要注意，应该先操作旧库，再操作新库。给业务方的返回值，以旧库为主，新库失败了没关系，切库之前不会对外提供服务，切库前，校验工具能够发现不一致，并修正。

------

**问：需求1->底层表结构变更：增加属性。不能直接在tb_user1上增加属性，必须要在tb_user2上增加属性吗？ 平滑迁移-双写法之步骤三：任何时间发现问题，大不了从步骤二开始重来是指数据不一致时，从步骤二开始重来？当校验数据一致性时，是让程序自动切到新库，还是人工手动切到新库，怎么切比较好？**

**答：** 1）增加属性可以，但如果变化比较多，还是倒库比较好。2）数据一致后，从旧库迁移到新库，可能配置的升级，例如：

- 2库配置变3库配置。
- 变更一个开关，双写变单写。

需要发消息或者重启的，不管哪种，都可以保证不影响可用性的。

------

**问：平滑数据迁移，核心代码会有侵入吗？文章中提到同时写两个库。**

**答：**不管哪个方案，都需要对服务进行升级，所以问题的答案是肯定的。

- 方案一，入侵较小，只需要加上一些日志。
- 方案二，相对方案一入侵会大一点，不过应该也还好，因为写接口是少数，需要多写一个新库。

------

**问：这样实现有什么问题：1）如果是添加字段，则使用文章《这才是真正的表扩展方案》的方式。2）如果是修改一种存储方式，分两次。第一次：将新的数据写入新的存储方式中，读取的时候单数据读取，如果获取失败，再获取旧的数据，列表读取。第二次：当历史数据已全部同步到新存储方式的时候，改变数据的读取方式为单数据源。**

**另外，需要一个同步小程序，把历史数据同步到新的存储方式中，并且在同步的时候，同步成功一条数据需要删除历史数据中的该条数据。**

**答：** 第一步增加1个字段，使用[《这才是真正的表扩展方案》](http://mp.weixin.qq.com/s?__biz=MjM5ODYxMDA5OQ==&mid=2651959765&idx=1&sn=b9916aa95c320e41035977e0a8098ca6&chksm=bd2d04098a5a8d1f3af38f658c05002151e621170949d2e3bb5b1bceea55c64b0477dba4c647&mpshare=1&scene=1&srcid=0322QVvYyMEPZret9IgjWQp1#rd)没有任何问题。

但如果有增有减，结构变化较大，还是导库吧.如果修改存储引擎，肯定是需要导数据的。

X库变Y库的非双倍扩容，可以使用文章中提到的两种方案平滑，升级，如果是成倍扩容，有更帅气的秒级扩容方案：[《数据库秒级平滑扩容架构方案》](http://mp.weixin.qq.com/s?__biz=MjM5ODYxMDA5OQ==&mid=2651959883&idx=1&sn=e7df8510c7096a5b069e0f12eaaca010&chksm=bd2d07978a5a8e815c2ae41b16b6b4c579923502fb919008a22bb108a1e920109f25387f8903&mpshare=1&scene=1&srcid=0322adn9pM9hMpUF7kW6unoO#rd)但这个方案只能实现 X库变2X库的扩容。

------

**问：说下mysql的分区表呢？**

**答**: 并发量较大，数据量较大的在线业务，互联网公司很少使用分区表（除非日志的按时间存储等少量需求）。

分库分表是把一个很大的库（表）的数据分到几个库（表）中，每个库（表）的结构都相同，但他们可能分布在不同的mysql实例，甚至不同的物理机器上，以达到降低单库（表）数据量，提高访问性能的目的。

分库分表往往是业务层实施的，分库分表后，为了满足某些特定业务功能，往往需要rd修改代码。

分区表是所有数据还在一个表中，但物理存储根据一定的规则放在不同的文件中。这个是mysql支持的功能，业务rd代码无需改动。

分区表有很多坑：

1. 分区表，分区键设计不太灵活，如果不走分区键，很容易出现全表锁。
2. 一旦数据量并发量上来，如果在分区表实施关联，就是一个灾难。
3. 自己分库分表，自己掌控业务场景与访问模式，可控。分区表，研发写了一个sql，都不确定mysql是怎么玩的，不太可控。
4. 运维的各种坑。

所以互联网在线高并发场景，不怎么使用分区表，而使用业务层自己分库分别。

------

**问： MySQL 在表DDL变更时长与表数量级，有没有一个合适的比例？**

**答：**:数据量大，并发量大，不做在线DDL，对数据库可用性有影响。

------

**问：58这边 mysql最高单表数据量可以到多少？**

**答：** 互联网业务特点，数据量大，并发量大，数据库访问没有各种复杂查询，基本1-2个过滤条件获得很少量的结果集，或者大部分是单记录查询，例如uid、order_id等，这种场景5kw单表没问题。如果业务访问模式更复杂一点，建议1kw-2kw数据就开始水平拆分。

------

**问：分库后，有做数据归档吗？还是一直水平扩展？**

**答：** 不归档，一直水平拆分。例如58同城100亿帖子，256库。

------

**问：最近做支付，老有误差，有啥好方法解决？**

**答：** 一致性是互联网架构的难点，数据量大，并发量大，多实例多机器数据分布。而分布式事务又非常难，所以我们能做的是，不能完全避免不一致，但尽快发现不一致并修复。

------

**问：大家平时说的分布式事务，多阶段提交，不同步骤之间通过RQ通信吗？**

**答：**如上题，分布式事务又非常难，所以我们能做的是，不能完全避免不一致，但尽快发现不一致并修复。多个步骤，可以直接调用，也可以MQ。

------

**问：你在文章中，经常提到，随机输入0-9的数字推荐好文，怎么实现的？**

**答：**这是一个非常好的问题，“架构师之路”回复大于10的任何整数，例如11，12，13，20，30，50，100，666，888，会返回一篇文章。

实现方式是秘密，哇咔咔，大家回复数据找找规律，看能不能把hash方法找出来。

------

**问：我们现在在做用户表的分库分表，考虑过双写法，但是由于用户表ID（自增）被用作其它业务的关联字段，无法保证两边同时写入的ID一致，所以我们分两步做：1）先分库，给主库建一个从库，停服，把从库提升为主库，再切服务，停服时间较短。2）在新库中建分表，借住trigger把插入和修改的同步到分表中，然后再写导数据工具把1张表里的数据导到100张表中，然后再切服务。感觉这么做有点没有效率，请问有什么更好的方法？**

**答：**自增ID不宜包含业务逻辑，如果用它来做了uid，兄弟，你踩上大坑了：

1. 无法扩容了。
2. 竞争对手很容易通过你们的uid推测你们业务增长量。

切记：自增ID不要包含业务含义，不要上游保存。

如果确实用了，我只能这么建议：把全系统的uid刷一遍？你这么导数据，自增ID会冲突吧？

------

**问：说一下rpc链路监控，日志这一块呢，有序性如何解决？**

**答**: 这是个很好的问题，我在业内大会讲过这个问题，但没有在公众号上写过。

有序性不能通过time来做，因为RPC调用链路是跨进程、跨机器的，各个机器的time不一致。 要设置一个时序id，跨进程调用的时候，时序id++来标识。

------

**问： select id,title,issue from t1 where title like 'plm%'; 因需求变动issue字段为null. null 字段会影响查询效率吗？**

**答**: SQL军规，少like哟，别NULL，不管这个SQL是否影响效率，NULL肯定会影响效率。参见[《58到家，数据库30条军规，与解读》](http://mp.weixin.qq.com/s?__biz=MjM5ODYxMDA5OQ==&mid=2651959906&idx=1&sn=2cbdc66cfb5b53cf4327a1e0d18d9b4a&chksm=bd2d07be8a5a8ea86dc3c04eced3f411ee5ec207f73d317245e1fefea1628feb037ad71531bc&mpshare=1&scene=1&srcid=0322D87logBtZEt8jOKPjlmD#rd)。

------

**问：系统使用了负载均衡，后端有4台服务器。错误日志记录在服务器上。不知道日志在哪台服务器，每次都得到4台服务器看一下。有啥好的解决方法？**

**答：**1）命令分发：在中控机执行命令，分发到N台机器，执行结果汇总返回中控机。2）日志汇总：很容易理解。

------

**问：58生产环境上，唯一性ID的生成用的是什么方案？**

**答：** 类snowflake方案，[《细聊分布式ID生成方法》](http://mp.weixin.qq.com/s?__biz=MjM5ODYxMDA5OQ==&mid=403837240&idx=1&sn=ae9f2bf0cc5b0f68f9a2213485313127&mpshare=1&scene=1&srcid=0322ClohJf35uMG5ZVB1Xzbv#rd)。N个服务，使用这个方法生成id，从配置中心拿不同的配置。

------

**问：今天主题是数据平滑迁移，沈大大你文章里说的应该是单库模式下数据服务切换，如果当前数据库采用多主多从(数据分片，比如8台数据库按照分片规则在写数据）这个时候数据迁移到新的数据库服务（可能多于8台服务），这个时候数据迁移小工具如何做到数据差异对比，新的服务何时切换比较合适？**

**答：**不是单库模式，多库没问题，倒库/追日志/比对 等小工具，配置好，从不同的库读就行。

------

**问：分布式环境中，在不改动代码的情况下，如何统计一次用户请求执行了多少条SQL语句，以及受影响的表的数量。**

**答：**可能我没有特别理解这个问题。从日志中不是可以看SQL么，以及对应的表么。

------

**问：在分库分表的交易系统中，收到A账户转B账户10块钱请求时，因为AB账户不在一个库，我这里的做法是A-10， B+10分2步, 为了保证幂等性（多次请求或网络抖动A-10不会重复提交），在账户表A库里有一个requst表，做A-10的操作时，同时保证这个事务内也要insert request记录（req id上层系统提供），然后记录该流水状态为处理中，然后再做B+10操作，一个事务保证在账户B的库里，B+10和insert B+10 request到request表都成功。这种处理有没有优化的余地，请问沈老师58有没有类似的场景，是怎么做的？**

**答：**这是一个很好的问题，也是很经典的问题。

1. 扣钱，加钱保证幂等。
2. 扣钱，加钱 各自保证消息可达性。

------

**问：一直在坐业务系统，虽然并发不高，各种复杂查询，尤其碰到变态业务的时候，几个大量的表的结合查询，性能比较头疼，一帮碰到这种情况下，SQL的优化思路是什么？**

**答：**我的经验，看slowlog，由性能最差的开始优化。



## 文章实录



### 一、问题的提出

互联网有很多“数据量较大，并发量较大，业务复杂度较高”的业务场景，其典型系统分层架构如下：

![enter image description here](http://7xpvay.com1.z0.glb.clouddn.com/807e1f70-084e-11e7-aa29-9dbe638c5d03)

1. 上游是业务层biz，实现个性化的业务逻辑
2. 中游是服务层service，封装相对通用的数据访问
3. 下游是数据层db，存储固化的业务数据

服务化分层架构的好处是，服务层屏蔽下游数据层的复杂性，例如缓存、分库分表、存储引擎等存储细节不需要向调用方暴露，而只向上游提供方便的RPC访问接口。当有一些数据层变化的时候，所有的调用方也不需要升级，只需要服务层升级即可。

互联网架构，很多时候面临着这样一些需求：

![enter image description here](http://7xpvay.com1.z0.glb.clouddn.com/8da40f20-084e-11e7-aa29-9dbe638c5d03)

**需求1->底层表结构变更**：数据量非常大的情况下，数据表增加了一些属性，删除了一些属性，修改了一些属性。

![enter image description here](http://7xpvay.com1.z0.glb.clouddn.com/92d89bf0-084e-11e7-aa29-9dbe638c5d03)

**需求2->分库个数变换**：由于数据量的持续增加，底层分库个数非成倍增加。

![enter image description here](http://7xpvay.com1.z0.glb.clouddn.com/9a676c70-084e-11e7-aa29-9dbe638c5d03)

**需求3->底层存储介质变换**：底层存储引擎由一个数据库换为另一个数据库。

种种需求，都需要进行数据迁移，**如何平滑迁移数据，迁移过程不停机，保证系统持续服务**，是文本将要讨论的问题。

### 二、停机方案

在讨论平滑迁移数据方案之前，先看下不平滑的停机数据迁移方案，主要分三个步骤。

![enter image description here](http://7xpvay.com1.z0.glb.clouddn.com/c7ba0de0-084e-11e7-aa29-9dbe638c5d03)

**步骤一**：挂一个类似“为了给广大用户提供更好的服务，服务器会在凌晨0:00-0:400进行停机维护”的公告，并在对应时段进行停机，这个时段系统没有流量进入。

![enter image description here](http://7xpvay.com1.z0.glb.clouddn.com/cc2c0e00-084e-11e7-aa29-9dbe638c5d03)

**步骤二**：停机后，研发一个离线的数据迁移工具，进行数据迁移。针对第一节的三类需求，会分别开发不同的数据迁移工具。

1. 底层表结构变更需求：开发旧表导新表的工具

2. 分库个数变换需求：开发2库导3库的工具

3. 底层存储介质变换需求：开发MongoDB导Mysql的工具

   ![enter image description here](http://7xpvay.com1.z0.glb.clouddn.com/d485a890-084e-11e7-aa29-9dbe638c5d03)

**步骤三**：恢复服务，并将流量切到新库，不同的需求，可能会涉及不同服务升级。

1. 底层表结构变更需求：服务要升级到访问新表
2. 分库个数变换需求：服务不需要升级，只需要改寻库路由配置
3. 底层存储介质变换需求：服务升级到访问新的存储介质

总的来说，停机方案是相对直观和简单的，但对服务的可用性有影响，许多游戏公司的服务器升级，游戏分区与合区，可能会采用类似的方案。

除了影响服务的可用性，这个方案还有一个缺点，就是必须在指定时间完成升级，这个对研发、测试、运维同学来说，压力会非常大，一旦出现问题例如数据不一致，必须在规定时间内解决，否则只能回滚。根据经验，压力越大越容易出错，这个缺点一定程度上是致命的。

无论如何，停机方案并不是今天要讨论的重点，接下来看一下常见的平滑数据迁移方案。

### 三、平滑迁移-追日志法

平滑迁移方案一，追日志法，这个方案主要分为五个步骤。

![enter image description here](http://7xpvay.com1.z0.glb.clouddn.com/dcf8e5a0-084e-11e7-aa29-9dbe638c5d03)

数据迁移前，上游业务应用通过旧的服务访问旧的数据。

![enter image description here](http://7xpvay.com1.z0.glb.clouddn.com/e6598ff0-084e-11e7-aa29-9dbe638c5d03)

**步骤一**：服务进行升级，记录“对旧库上的数据修改”的日志（这里的修改，为数据的insert、delete、update），这个日志不需要记录详细数据，主要记录：

1. 被修改的库
2. 被修改的表
3. 被修改的唯一主键

具体新增了什么行，修改后的数据格式是什么，不需要详细记录。这样的好处是，不管业务细节如何变化，日志的格式一定是固定的，这样能保证方案的通用性。

这个服务升级风险较小：

1. 写接口是少数接口，改动点较少

2. 升级只是增加了一些日志，对业务功能没有任何影响

   ![enter image description here](http://7xpvay.com1.z0.glb.clouddn.com/ec48e140-084e-11e7-aa29-9dbe638c5d03)

**步骤二**：研发一个数据迁移工具，进行数据迁移。这个数据迁移工具和离线迁移工具一样，把旧库中的数据转移到新库中来。

这个小工具的风险较小：

1. 整个过程依然是旧库对线上提供服务
2. 小工具的复杂度较低
3. 任何时间发现问题，都可以把新库中的数据干掉重来
4. 可以限速慢慢迁移，技术同学没有时间压力

数据迁移完成之后，就能够切到新库提供服务了么？

答案是否定的，在数据迁移的过程中，旧库依然对线上提供着服务，库中的数据随时可能变化，这个变化并没有反映到新库中来，于是旧库和新库的数据并不一致，所以不能直接切库，需要将数据追平。

哪些数据发生了变化呢？步骤一中日志里记录的不就是么？

![enter image description here](http://7xpvay.com1.z0.glb.clouddn.com/f31943c0-084e-11e7-aa29-9dbe638c5d03)

**步骤三**：研发一个读取日志并迁移数据的小工具，要把步骤二迁移数据过程中产生的差异数据追平。这个小工具需要做的是：

1. 读取日志，得到哪个库、哪个表、哪个主键发生了变化
2. 把旧库中对应主键的记录读取出来
3. 把新库中对应主键的记录替换掉

无论如何，原则是数据以旧库为准。

这个小工具的风险也很小：

1. 整个过程依然是旧库对线上提供服务
2. 小工具的复杂度较低
3. 任何时间发现问题，大不了从步骤二开始重来
4. 可以限速慢慢重放日志，技术同学没有时间压力

日志重放之后，就能够切到新库提供服务了么？

答案依然是否定的，在日志重放的过程中，旧库中又可能有数据发生了变化，导致数据不一致，所以还是不能切库，需要进一步读取日志，追平记录。可以看到，重放日志追平数据的程序是一个while(1)的程序，新库与旧库中的数据追平也会是一个“无限逼近”的过程。

什么时候数据会完全一致呢？

![enter image description here](http://7xpvay.com1.z0.glb.clouddn.com/fa620a90-084e-11e7-aa29-9dbe638c5d03)

**步骤四**：在持续重放日志，追平数据的过程中，研发一个数据校验的小工具，将旧库和新库中的数据进行比对，直到数据完全一致。

这个小工具的风险依旧很小：

1. 整个过程依然是旧库对线上提供服务 （2）小工具的复杂度较低 （3）任何时间发现问题，大不了从步骤二开始重来 （4）可以限速慢慢比对数据，技术同学没有时间压力

   ![enter image description here](http://7xpvay.com1.z0.glb.clouddn.com/006299f0-084f-11e7-aa29-9dbe638c5d03)

**步骤五**：在数据比对完全一致之后，将流量迁移到新库，新库提供服务，完成迁移。

如果步骤四数据一直是99.9%的一致，不能完全一致，也是正常的，可以做一个秒级的旧库readonly，等日志重放程序完全追上数据后，再进行切库切流量。

至此，升级完毕，整个过程能够持续对线上提供服务，不影响服务的可用性。

### 四、平滑迁移-双写法

平滑迁移方案二，双写法，这个方案主要分为四个步骤。

![enter image description here](http://7xpvay.com1.z0.glb.clouddn.com/099c8260-084f-11e7-aa29-9dbe638c5d03)

数据迁移前，上游业务应用通过旧的服务访问旧的数据。

![enter image description here](http://7xpvay.com1.z0.glb.clouddn.com/0e594720-084f-11e7-aa29-9dbe638c5d03)

**步骤一**：服务进行升级，对“对旧库上的数据修改”（这里的修改，为数据的insert、delete、update），在新库上进行相同的修改操作，这就是所谓的“双写”，主要修改操作包括：

1. 旧库与新库的同时insert
2. 旧库与新库的同时delete
3. 旧库与新库的同时update

由于新库中此时是没有数据的，所以双写旧库与新库中的affect rows可能不一样，不过这完全不影响业务功能，只要不切库，依然是旧库提供业务服务。

这个服务升级风险较小：

1. 写接口是少数接口，改动点较少

2. 新库的写操作执行成功与否，对业务功能没有任何影响

   ![enter image description here](http://7xpvay.com1.z0.glb.clouddn.com/16a721e0-084f-11e7-aa29-9dbe638c5d03)

**步骤二**：研发一个数据迁移工具，进行数据迁移。这个数据迁移工具在本文中已经出现第三次了，把旧库中的数据转移到新库中来。

这个小工具的风险较小：

1. 整个过程依然是旧库对线上提供服务
2. 小工具的复杂度较低
3. 任何时间发现问题，都可以把新库中的数据干掉重来
4. 可以限速慢慢迁移，技术同学没有时间压力

数据迁移完成之后，就能够切到新库提供服务了么？

答案是肯定的，因为前置步骤进行了双写，所以理论上数据迁移完之后，新库与旧库的数据应该完全一致。

由于迁移数据的过程中，旧库新库双写操作在同时进行，怎么证明数据迁移完成之后数据就完全一致了呢？

![enter image description here](http://7xpvay.com1.z0.glb.clouddn.com/3ca47960-084f-11e7-aa29-9dbe638c5d03)

如上图所示：

1. 左侧是旧库中的数据，右侧是新库中的数据
2. 按照primary key从min到max的顺序，分段，限速进行数据的迁移，假设已经迁移到now这个数据段

数据迁移过程中的修改操作分别讨论：

1. 假设迁移过程中进行了一个双insert操作，旧库新库都插入了数据，数据一致性没有被破坏
2. 假设迁移过程中进行了一个双delete操作，这又分为两种情况
   - 假设这delete的数据属于[min, now]范围，即已经完成迁移，则旧库新库都删除了数据，数据一致性没有被破坏
   - 假设这delete的数据属于[now, max]范围，即未完成迁移，则旧库中删除操作的affect rows为1，新库中删除操作的affect rows为0，但是数据迁移工具在后续数据迁移中，并不会将这条旧库中被删除的数据迁移到新库中，所以数据一致性仍没有被破坏
3. 假设迁移过程中进行了一个双update操作，可以认为update操作是一个delete加一个insert操作的复合操作，所以数据仍然是一致的

除非除非除非，在一种非常非常非常极限的情况下：

1. date-migrate-tool刚好从旧库中将某一条数据X取出
2. 在X插入到新库中之前，旧库与新库中刚好对X进行了双delete操作
3. date-migrate-tool再将X插入到新库中

这样，会出现新库比旧库多出一条数据X。但无论如何，为了保证数据的一致性，切库之前，还是需要进行数据校验的。

![enter image description here](http://7xpvay.com1.z0.glb.clouddn.com/44d8d8b0-084f-11e7-aa29-9dbe638c5d03)

**步骤三**：在数据迁移完成之后，需要使用数据校验的小工具，将旧库和新库中的数据进行比对，完全一致则符合预期，如果出现步骤二中的极限不一致情况，则以旧库中的数据为准。

这个小工具的风险依旧很小：

1. 整个过程依然是旧库对线上提供服务

2. 小工具的复杂度较低

3. 任何时间发现问题，大不了从步骤二开始重来

4. 可以限速慢慢比对数据，技术同学没有时间压力

   ![enter image description here](http://7xpvay.com1.z0.glb.clouddn.com/4ae32c10-084f-11e7-aa29-9dbe638c5d03)

**步骤四**：数据完全一致之后，将流量切到新库，完成平滑数据迁移。

至此，升级完毕，整个过程能够持续对线上提供服务，不影响服务的可用性。

### 五、总结

针对互联网很多“数据量较大，并发量较大，业务复杂度较高”的业务场景，在

1. 底层表结构变更
2. 分库个数变换
3. 底层存储介质变换

的众多需求下，需要进行数据迁移，完成“**平滑迁移数据，迁移过程不停机，保证系统持续服务**”有两种常见的解决方案。

**追日志法**，五个步骤：

1. 服务进行升级，记录“对旧库上的数据修改”的日志
2. 研发一个数据迁移工具，进行数据迁移
3. 研发一个读取日志小工具，追平数据差异
4. 研发一个数据比对小工具，校验数据一致性
5. 流量切到新库，完成平滑迁移

**双写法**，四个步骤：

1. 服务进行升级，记录“对旧库上的数据修改”进行新库的双写
2. 研发一个数据迁移工具，进行数据迁移
3. 研发一个数据比对小工具，校验数据一致性
4. 流量切到新库，完成平滑迁移
5. ​