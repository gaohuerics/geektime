## 概要

#### 分享人简介

- [庄表伟](http://gitbook.cn/m/mazi/author/57e37d2569efa0cb63915b7b)，华为公司内源社区平台架构师，开源社理事。
- [王渊命](http://gitbook.cn/m/mazi/author/5822db49168544c55856ad73)，QingCloud 容器平台负责人，前新浪微博架构师。
- [肖德时](http://gitbook.cn/m/mazi/author/584e89062fac8c6f04da90d1)，曾任Redhat Engineering Service部门内部工具组Team Leader，是国内第一代 Docker 代码贡献者。
- 陈皓（@左耳朵耗子），前阿里和亚马逊的技术主管。GitChat技术顾问，本场Chat特约嘉宾。

#### 案例背景

**庄表伟**：这个案例，是我在工作中遇到的具体的问题，我们在公司里，需要建设一个内部的“开源社区”，大家可以理解为一个面向华为内部员工共同使用的Github。从2013年底开始建设，到现在3年多的时间。在当时，我们能够选择的基础平台，其实非常有限，最合适的，也就是Gitlab的开源版本。至于后来出现的Go的Gogs、PHP的Phabricator，当时都还没有，再加上我对Ruby也比较熟悉，就自然选择了Gitlab。而Gitlab，是一个Rails的项目，因此我们的平台，原生就是带着Rails基因的。

华为目前的研发人员，超过8万人，另外还有数量非常多的外包团队，因此，要提供一个足够供他们使用的平台，分布式架构是一个必然的选择。随着Docker与微服务技术的逐渐成熟，对于Gitlab的微服务架构改造，也就成为我们思考的重要问题。

![enter image description here](http://7xpvay.com1.z0.glb.clouddn.com/1b3e8af0-02d6-11e7-8113-8bf2f0ddb563)

这是目前Gitlab的架构图，不过我们在对Gitlab做二次开发的时候，还没有Gitay这个组件。另外数据库，用的也不是PostgreSQL，而是MySQL。

另外，<https://gitlab.com/gitlab-org/gitlab-ce/blob/master/db/schema.rb> 这个目前Gitlab的表结构，一共有78张表。我们在后续的开发过程中，添加了不少，目前还在做梳理，大致上会保留103张表。

目前对于数据库中表的分类主要包括：

- Platform：平台的大多数其他表。
- Project：与组织、项目相关的所有信息，这一类下面还有2个子类：
  - Code：与代码相关的各种信息。
  - Issue：与Issue相关的各种信息。

另外我们的扩展还包括一些表：

- Group： 与讨论组相关的各种表。
- Social：与社交行为相关的各种表。

所以，初步的想法是从大的平台中，分离出：Project、Group、Social三类服务，剩下的留着不动。

------

**问：监控系统如何监控自己？如果监控自己都没有很好的运作，那么当我们谈到调用链监控的时候是不是存在不确定性？**

**肖德时**: 这是用健康检查机制来保证的，坏了会报警。

**王渊命**：监控与报警系统其实一般是两个系统，报警系统依赖于健康检查以及监控系统的输出的数值。监控系统不正常工作，顶多是发现问题的时候排查诊断有一定困难，但没监控系统你还是要能人肉排查的吧。

**庄表伟**：我也想说说我的理解：监控也是分层次的，不同层次的监控，是逐级往上监控的。对于最基础的监控：比如机房是不是有电，这个也需要，而且一定得靠不是互联网的手段来报警。

我还有一个联想：在分布式系统中，多个实例的选举机制。也是互相监控的情况。

------

**问：阿里云推出了容器服务，采用他们容器服务是不是会降低微服务的门槛？**

**肖德时**： 我认为不会。微服务的设计和部署都很重要，但是架构设计更能体现价值。

**王渊命**：我觉得直接使用容器服务可以降低容器服务本身的维护门槛，但容器只涉及微服务的运维管理问题，至于如何架构，如何拆分，服务如何治理，还是有很多门槛的。

**庄表伟**：其实，我倒是想提一个类似的问题：比如完全用docker原生的技术，还是用kubernets的平台，是不是会降低微服务化改造的难度？

**陈皓**：容器平台不会降低门槛，因为微服务的重点不在PaaS，而是在自己怎么拆分自己的业务，还有自己业务的架构。微服务更多说的是业务架构、业务应用生命周期的调度和业务应用间的关系。打个比方：微服务是汽车，容器平台是公路，公路是让汽车跑得更好，但公路再好不会降低汽车的生产门槛。：）

**王渊命**：我是比较看好容器平台对微服务的支持的，只是现在容器平台上层的工具链还不够成熟。比如服务治理，监控，以及服务注册，这些都可以让渡给容器平台的。

**肖德时**：解偶过程有什么经典坑，有这么一个，就是一个服务启动后依靠多个其他子服务。

**陈皓**：根据程序的本质：程序 = 控制Control + 逻辑 Logic这个Principle来看，所谓控制，就是我们程序对数据和流程的控制（比如：分布式、并行等），逻辑就是业务逻辑，Control是程序复杂度的下限，而Logic是程序复杂度的上限。就是说，如果业务逻辑本身就复杂，我们是无法通过程序的手段降低复杂度的。Docker和K8S就是控制，而业务逻辑才是整个系统复杂度的根源。我们经常把控制和逻辑混在一块，导致程序或架构更加的混乱和复杂。

------

**问：微服务化之后，系统管理后台和app使用的微服务的业务代码需不需要分开实现。gateway需不需要分开？**

**王渊命**：你的这个gateway是API gateway么？这个肯定要分开的吧。但管理后台，一种是直接用业务代码的API，好处是只维护一套API，一套业务逻辑，缺点是需要给管理后台做许多专门的API。

如果管理后台单独实现，直接操作数据库，这种是比较方便，但缺点是以后数据库升级就比较麻烦，缓存相关的处理也比较麻烦。如果团队大了，最终会切换到前面那种方式。

**提问者：**是API gateway。

**王渊命**：还有一种办法是管理后台的读操作直接读取一个从库，但写操作需要通过业务api。这种管理后台开发人员的灵活性比较高，可随意定制，也一定程度考虑到了数据一致性问题。

**庄表伟**：这种搞法，似乎更累啊。

**陈皓**：个人觉得：不管是不是微服务，管理后台都是要和用户的业务分开的，从安全的考虑也应该是这样的。

**问：**但是如果把管理后台和app的功能放到一起，权限方面怎么控制，系统用户和会员是放到一起，还是分开设计？因为我们做的都是一些传统应用。

**庄表伟**：嗯，是说最后一种。

**王渊命**：我觉得他问的是API层，这个其实挺矛盾的。比如 user service，同时也给管理后台输出接口么？这个我觉得应该按照API认证和权限机制来控制。

**庄表伟**：在我们的系统里，管理后台的任务，还不算繁重，所以并没有特别的处理。但是，如果有非常多的管理、配置、权限之类的逻辑，似乎独立一个管理服务，会更加妥当些。但是，这个管理的服务，应该会调用其他业务的API，而不是搞读写分离吧。

**陈皓**：从业务和安全的角度来讲，像权限比较大的专人使用的API，是另一个网关。

**王渊命**：没办法，管理后台的需求变化非常大，如果全部要业务团队来输出api，会比较累，时间上也会有问题。所以直接给个从库搞也是一种妥协的办法。我是按照互联网应用的需求来分析的。

**陈皓**：我觉得不要把什么事都往微服务上来靠，微服务不核心，只是一个手段，核心是业务。所以，还是要从业务上来挑选不同的手段。

**庄表伟**：我插一句，聊聊我们的系统的思考。基本上，我是打算，把能分的分出去。剩下的还是都放在一起。而且，之前肖德时提到的判断标准，我觉得很有道理：有横向扩展压力的业务，才需要拆分出去，作为微服务。

**王渊命**：没有完美的方案，要根据业务阶段选择。比如当前业务紧张，用户需求开发团队都忙不过来，管理后台的需求就耽搁了，但有些管理后台的需求又是运营团队和管理团队必须的（比如内容审核 等等）。这时候就只能用一些妥协的，可能违反设计原则的办法了。

**问：**商城一般后台有系统用户管理，会员管理，商品分类，商品管理，订单管理等。商城的商品管理和订单处理引擎之间的关系？

**王渊命**：我理解你的意思是，比如我们用了 用户和订单微服务，这个是直接面向最终用户的api，输出给web和app，这时候管理后台需要管理用户以及订单，这部分接口和用户的接口有部分重复，但也有一部分转有的，这部分应该放在 用户和订单的微服务里，还是单独搞管理后台的微服务，是否共享数据库。我前面的回答都是按照这个理解回答的。

**问：**我的问题主要是后台这部分功能，在微服务里面应该怎么放，是合在一起，还是分开，如果合在一起，那么api 网关应不应该合在一起还是分开。因为我看了很多微服务的分析架构，但是都没有讲到管理后台这么分怎么开发。

**陈皓**：我们先不说微服务，我们先说SOA，对于电商这样的平台，服务也需要分层次，有后端服务（如：数据model层的），也有中台服务（如：业务和技术中间件），还有前端服务（提供给用户的服务），对于这三个层次，后端和中台的是可以共享的，前端的建议分开，gateway也应该分开。

商城的商品管理和订单处理，这应该是两个业务，我不知道你说的关系具体是什么，能具体说一下吗？

**问：**商品与订单流程引擎看上去是分开的模块，但在运行时是紧密相关的，比如订单处理过程经常要读到商品的各种特殊属性来做不同的逻辑，所以修改商品处理流程实际上需要同时修改商品管理模块的流程管理模块，那么多次修改之后非常容易让两个模块之间偶合得很紧，那么当初解偶到微服务的意义 就大减，那么怎么去避免这种偶合问题？

**陈皓**： 按我的理解，一旦订单下了，应该对下单的商品做一个“小快照”和reference，因为商品的信息可能会经常变化的，所以，一些和订单的相关的数据必需要下单时持久化在订单内（比如：价格、折扣、商品的基本信息和基本属性，等等），这主要是怕用户下单时和之后的商品数据变化产生不一致的情况，所以需要把相关数据冗余到订单上来。当然，订单中心也需要反向查一些商品的信息，但是从业务上来说，这样的情况一般来说是对不影响订单的状态的，所以，从业务上来说，这应该是一个从业务上来说就是松耦合的两个系统。所以，其实这个还是业务问题，这就是微服务最难的地方——业务分析能力。

**提问者：**以我的理解，管理后台这么错综复杂的东西，如果业务量不是特别巨大，那么只需要把数据库、缓存、消息队列等基础服务分出去，如果某个无状态的API业务量巨大，那么就应该把它单独分出来能横向扩展。

**陈皓**：你说的是技术中间件，还有业务中间件，也是可以共享的（比如：权限中心、短信中心、支付服务之类的）。

**肖德时**：商城一般后台有系统用户管理，会员管理，商品分类，商品管理，订单管理等。我见过的大多数就是单体应用来支撑就够了。没有流量。落数据库就可以了。

------

**问：微服务接口的版本如何控制冗余比较好？比如有接口需要升级，但又要考虑对老接口调用的兼容，代码层面如何实现或部署比较好，有没有什么标准规则？**

**肖德时**：微服务的网关做路由到新旧两个版本的实例上。

**庄表伟**：关于这个问题，我先抛个砖，这种情况，不是一般都是在API里，加上v1,v2这样的路径，就可以了吗？

**王渊命**：这种方式新版本的接口其实相当于一个新接口，对老接口完全没影响。但这种做法也有成本，就是版本不好控制，有的接口新老版本是一样的，有的是不一样的。某一个接口需要升级新版本的时候，其他接口怎么办？在网关层路由到旧接口？

我觉得大多数情况下，接口的参数以及返回值变动都不应该作为新版本，一般在代码层作为兼容，除非有非常大的变更，需要完全重新设计API。

**庄表伟**：我觉得，你调用一个接口，就存在一种依赖。无论怎么处理，依赖总是个麻烦。所以：把麻烦显式的暴露出来，还更好一些。（所以v1,v2就不错）

**陈皓**：关于微服务接口的版本兼容，无论有没有微服务都是需要考虑的。这个可以学习一下各个公司的API设计的方式和维护的经验，google一下也有很多相关的规范文章。

------

**问：微服务中所有前后端交互是否都要走API gateway？**

**肖德时**：可以走，也可以不走。看你的数据流向。开始的时候，你可以全部放到API Gateway。

**王渊命**：我个人认为 api gateway 是对外输出的，服务之间可以直接通过服务发现互通。

------

**问：微服务之间是否会存在依赖关系？如何管理？**

**肖德时**：有API联动就有依赖，这个依赖是一种病毒。一定要优化避免。

**王渊命**：会有依赖关系，我文中建议的方式是进行依赖分层，避免依赖循环。复杂情况下，你可能需要一个聚合层来聚合各服务的api数据然后组装成最终对外输出的结果。

**陈皓**：关于依赖关系，有很多pattern可以解开依赖，一种是IoC/DIP的设计思路，变业务间的依整为所有的服务都依整一个统一的协议。比如一个Pub/Sub的消息/数据总线（当然，这是依赖变得复杂时的玩法）。

**问：**关于依赖我有个简单例子，比如发送手机验证码修改密码，修改密码这个服务我请求后，必定会联动后面的短信网关服务，发送验证码到用户手机上，然后让用户完成修改密码这个操作。当然这只是一个例子，碰到这种业务，如何避免依赖调用？

**王渊命**： 你的这种依赖是正常的啊。不然要微服务何用。

**肖德时**：我所说的依赖是你的API服务，因为其他子API，变的不可用了。我所说的依赖是你的API服务，因为其他子API，变的不可用了。因为微服务下，很难调试。当你有几十个微服务的情况下，调用链条就多了很多，你无法快速定位。所以，从API的单一原则来讲，必须保障API的服务的可用性。

**王渊命**：依赖一个是要有跟踪机制，另外一个是避免循环依赖。

**提问者**：有zipkin就可以，可惜不是dubbo里的，springcloud提供的。

**肖德时**：zipkin是工具，但是不是灵丹妙药，一定要保持简单原则。

**王渊命**：服务多了，很容易循环依赖，不小心一个调用，就相当于内网 ddos了。

**陈皓**： 发短信这个服务是个无状态的服务，也就是一个什么都不知道的，别人让我干啥我就干啥的服务。而发验证码这个事，是用户认证系统要干的事，也就是说，用户认证系统生成验证码，然后调用“发验证码服务”，而“发验证码服务”会调用更为公用的的“发邮箱服务”或“发短信服务”。这也是一种解耦的方式“依整于接口而不是实现” （接口其实就是一种协议）。

------

**问：赞同在对于API和WEB的拆分时，model层单独抽成服务，不过对于这种服务该怎么去建设呢，是对其他服务提供API调用还是其它的，这一块的扩展该怎么考虑？**

**王渊命**：model层不应该算单独的服务吧，应该是一个 lib 由其他服务依赖吧？model其实可以理解成服务之间的数据传输的 schema。用 protocolbuffer 定义然后各服务自己生成也一样吧。

------

**问：器编排系统开放给应用开发人员使用时，️比较成熟的权限管理方案吗？Gitlab后端采用的网络文件系统选型时有考虑过glusterfs吗？为什么最终采用了nfs呢？优秀实践能稍展开分享下吗？**

**肖德时**：这个问题好几个串起来，我回答容器编排加权限，这块目前都是自研的。用成熟的方案就可以，比如Django。

**庄表伟**：分2个部分。第一个部分关于权限的，我也想了解。先回答后一个部分。因为git是一个配置库，如果有些文件缺失，会导致整个配置库的数据错误。不像普通的分布式文件系统。我们可以等等，晚点再看。所以，git的配置文件的同步，我们现在就是使用git自己的同步协议。因此，想来想去，没法用现成的分布式文件系统。

------

**问：对于服务间的通信请问应基于哪些考虑来选择REST还是rpc，且服务间的通信需要考虑哪些安全信息？**

**王渊命**：我觉得有几个考量标准：

1. qps以及接口输出，如果非常大的量级，rpc还是有优势的。
2. 开发语言，如果开发语言有不支持 rpc 的，只好用 REST 了。

另外 rpc 库一般都支持服务发现中心，直接通过 smartclient 模式互相直接连接，以及进行流量切换，REST 一般还是需要个 lb来做这个事情。

**肖德时**：restful可以，遇到性能问题在rpc，内部数据调用用rpc合适，但是年轻程序员不一定能搞定protobuf之类，还是用httprestful.可以。安全，有jwt之类，很多方案，没有什么特别的。

**陈皓**：任何事情都是trade off， RPC有很多问题，且不利用服务的各种调度。但是RPC可以提高性能，所以，建议使用RPC时，最好不要使用很长很长的链接，而且需要使用一个RPC的中件间或proxy。

------

**问：想请问一下，什么的样的场景适合微服务？大家的判定标准是什么？**

**肖德时**：分布式场景下才会有这个问题，比如我做的容器云平台，底下是分布式系统，所以我就利用这个微服务加快构建体系。这个叫因地制宜。还有一种是当单体业务或者SOA的业务依赖太重了。期望加快一个单体应用中的一个功能的水平扩展的时候，就可以考虑加入微服务模块。

**王渊命**：一般是纵向伸缩出现瓶颈的时候才需要微服务化。还一个笑话说，如果你的几个团队互相看不上，你就得拆成微服务了，也就是团队协作原因。

------

**问：现在Docker只是让部署更快吧？和解偶没关系，那么解偶过程有什么经典坑可以分享一下？**

**陈皓**：Docker的强项除了环境部署，环境隔离，还有集群调度（自动化调度的前提是自动化部署），Docker和业务解耦没什么关系。业务解耦好处很多，我做业务这么多年，解耦最大的坑就是“程序控制逻辑会比较复杂，尤其是业务的流转状态”，我看到很多公司都把一个订单的状态机放到了各种不同的模块里去，结果每个模堆块维护整个状态机的一部分，最终导致状态混乱，各种bug和问题，这应该是最大的一个问题。要解决这个问题，其实是另一个话题了，可以放在以后的chat中讲。

------

**问：如何来设计微服务之间的访问和通信是采用同步还是异步的原则，同步和异步访问有比较好的介质么（中间件）？**

**王渊命**：同步模型就是通用的 rpc/rest模式，异步模型一般是 actor 模型。有许多现成的框架。

**庄表伟**：关于同步、异步的理解：“如果没有实时的业务压力，能够异步的就异步”，我这样说对吗？

**提问者**：我觉得非必要，不要搞太多异步的，异步带来出错难跟踪，就这一个问题就可以搞死人。

**王渊命**：除非有好的 actor 框架把复杂度屏蔽了。

**庄表伟**：嗯，我也是抛一个观点来大家聊：假设已经有一个很稳定的消息队列+任务总线呢？是不是会更加自然的把任务丢到异步的队列里去？

**提问者**：现代系统本身都是消费者远多于CPU数量的，异步带来消费者不需要等待，但worker线程这个最费资源的单点没解放出来。除非是用户不想等待，否则不要经常引入异步机制。

**肖德时**：假设已经有一个很稳定的消息队列+任务总线呢？是不是会更加自然的把任务丢到异步的队列里去？ 肯定会。银行的业务百试不爽，天天再用。因为到后端以后，银行的业务并不是高并发容量。所以不需要微服务。但是，随着互联网业务出来后，就不行了。

**王渊命**：异步队列这种和前面说的微服务场景不一样，异步队列这种写入就算成功了，不会等待结果。如果微服务的远程调用做成异步的，调用后，对方是要异步通知到调用方的。场景不一样。

**肖德时**：怎么讲，业务量真正的扩大了。异步解决不了业务上的一个完整交易。必须一次性完成。这个就瞎了。所以你会看到很多金融的业务在不断的当机中度过难关，找互联网企业找方案借鉴。

------

**问：工作流引擎作为基础模块，应该是可以作为单独的微服务的，但相关的业务又是在另外的微服务中，但工作流流转过程中要频繁的调用业务的微服务，而业务逻辑中也有可能调用工作流的服务，这其中涉及的性能问题和事务问题如何平衡？或者说这种拆分是不是有问题？**

**肖德时**：首先这里就是循环依赖了。微服务已经起不到效果。要保证业务逻辑的设计不要循环依赖。应该在一起的就放在一起。别搞微服务。你可能都不需要扩展实例。

假设已经有一个很稳定的消息队列+任务总线呢？是不是会更加自然的把任务丢到异步的队列里去？ 肯定会。银行的业务百试不爽，天天再用。

因为到后端以后，银行的业务并不是高并发容量。所以不需要微服务。但是，随着互联网业务出来后，就不行了。 怎么讲，业务量真正的扩大了。异步解决不了业务上的一个完整交易。必须一次性完成。这个就瞎了。所以你会看到很多金融的业务在不断的当机中度过难关，找互联网企业找方案借鉴。

**陈皓**：正好我在Amazon工作过，Amazon是个超级喜欢Workflow的公司。Workflow的微服务的实现，可以参考Amazon在AWS上的SWF - Simple Workflow。简单来讲这个Service有如下几个组件供你参考：

1）Workflow: 这个东西就是引擎，其用来记录整个工作流当前的工作历史，并把工作流执行的历史和相当的数据上下文发送给Decider，这个组件也是无状态的。

2）Decider：这个东西分析Workflow传过来执行历史和相关的数据上下文，并根据用户定义好的流程来做出下一步应该做什么的决定，并把决定交给Workflow。

3）Worker：各种各样的干活的Worker，完全无状态。定期的以long poll的方式从workflow拉取要干的活，并回传结果。

------

**问：有关api网关一个问题，是否除了鉴权，sso的功能也放在网关做，目前如果要支持原生app的权限，是否只能用token-base方式还是继续用传统的cookie-base方式？另外一个问题，本人一直觉得使用rpc开发微服务是种非最佳实践的方式，那么在微服务体系中是一定要有rpc，还是可以不采用？不采用的话我们该如何做？**

**王渊命**：我觉得网关做鉴权没问题，但sso这个本身应该是个服务吧，不应该是网关的。app 一般肯定是 token 机制啊，cookie是浏览器的机制没办法，app又不受这个限制。rpc 可以看前面关于 rpc 和rest的分析吧。

**陈皓**：1）API的鉴权设计有很多方式，这个其实和API的设计有关系，也和安全有关，个人觉得这个话题放到未来的API设计中去吧。

2） RPC是不是微服务的最佳实践，我给两个不同的案例：Amazon内部是拒绝RPC的通通走web service，而且Amazon是喜欢pull模型而不是push模型。阿里内部是喜欢RPC的，因为需要性能，而且中间件一开始也用push模型，但后来变成了pull模型。 （我个人喜欢Amazon的方式。）

------

**陈皓**：两个小时就这样过去了。大家的问题还有很多，我们不可能回复这么多的问题。很感谢大家的问题和讨论。我也是第一次做这个事，所以，有点喧宾夺主，三位嘉宾见谅了。在这里我简单总结一下：

1. 业务上的分析和解耦能力
2. 相关的软件设计的基础

还是性能优先，还是高可用优先……这样才能设计出合理的架构（个人建议应用的拆解应该先从不同的业务分离开始，然后进化到业务中间件的出现，再到技术中间件，然后再到SOA。）只有当我们的业务架构进化到了可以微服务的时候，才能开始进入微服务的领域。



## 文章实录



### 一、背景介绍 by 庄表伟

**1.** 原始架构是Gitlab，一个开源的github clone。基于rails框架开发的Web项目，Rails+MySQL+Sidekiq+Redis+LocalFileSystem。如果不对Gitlab做架构改造，在负载与存储量不断上升之后，只能是在Nginx之后，运行多个Rails Web实例，通过NFS mount到后端的一个超大的NAS存储，然后MySQL与Redis部署成独立服务。最终这个方案，将难以为继。

**2.** 首先做的改造，是存储分片，将不同的git仓库，存储在多台不同的存储服务器上，形成N*M个NFS mount连接。再后来，我们将`gitlab_git`（一个gitlab的git封装）改造成rpc模式，前端Web使用`gitlab_git_client`，访问后端的`gitlab_git_server`。中间的访问协议是dRuby，访问路由存储在redis里。

**3.** 由于企业内部对于网络传输速度的要求很高，从Github下载代码，500K/s已经非常理想，但是在公司内，8~10M/s这个水平才是可以被接受的。在跨地域读取的情况下，下载速度有8~20倍的差距，迫使我们必须做多数据中心的部署。考察过各种分布式文件系统，但是性能都无法保障，只能自己做：按项目路由、分布式存储、自建备份能力。

**4.** 目前建了N个数据中心，共计超过100台服务器。

**5.** 目前的数据库，主要是MySQL，还有一些存在MongoDB里，总量超过200G，且增长迅猛。

**6.** 我们目前主要在用Sidekiq做任务队列，每天的任务数量大约1~2百万。之前有一个量非常大的邮件群发任务，目前已经分拆出去，基于RabbitMQ，做了一个独立的邮件发送服务。

**7.** 在自动化部署方面，目前主要是通过chef+ansible来做的，chef安装软件；ansible是把依赖软件跟应用的安装流程串起来；曾经在docker swarm与kubernetes之间徘徊，后来决定不再考虑docker swarm。

**8.** 在监控方面，购买了一个商业产品，自己也有一些简单的基于grafana+influxdb的监控。

**9.** 在日志搜集与分析方面，目前还是通过Logstash+ElasticSearch做了一个简单的架子。

### 二、思考与选择 by 庄表伟

**1.** 经过各种比较分析，最终我还是选择了Kubernetes，主要是2 个原因：

- Services与RC的概念划分，使得我们不必太多考虑伸缩性的问题。
- POD与Container的概念划分，使得我们的每一个镜像都是简洁的，不会搞成每个image里，都要加一个nginx的事情。
- **肖德时注解**：Kubernetes是很优秀的分布式容器集群，解决的是应用纬度的问题，需要实践的地方很多。所以专门的运维平台组的建立是有必要的。另外，Rancher这个项目也可以参考。镜像仓库可以考虑Harbor。
- **肖德时注解**：还要[培训和自我学习](https://zhuanlan.zhihu.com/p/25325482)。

**2.** 在镜像制作方面，我现在在琢磨基于alpine 3.4（4.8M），打造一组最小化的镜像。但是不确定是不是“邪道”。

- 另外，也想请教一下，是否有一套成熟的镜像继承树的最佳实践？
- **肖德时注解**：开发测试集成环境，有一个开发用的Base作为基础。把需要的依赖都加入。使用compose启动本地依赖环境。
- **肖德时注解**：生产用镜像使用轻量级镜像入alpine，对于ruby场景使用phusion/baseimage。

**3.** 关于API，情况比较复杂，需要多说两句：原始的Gitlab项目中，自带了grape的API框架，API的代码与Web的代码是放在一起的，共用一套model定义，因此，最初我做的拆分，是在rails之外，独立做了一个API Server，是基于sinatra+sequel的。但是，因为数据库结构的不断变动，尤其是各种关联关系、触发条件的机制，使得另外维护一套model很不划算，再加上权限管理也很复杂。所以，我们现在的API还是和Web服务是放在一起的。

- 因此，如何将API逐一拆分出来，我这边还感觉比较困惑。
- **肖德时注解**：使用eBay/fabio在API Server之前挡住真实的单体应用API，然后把具体的一个模块重写。通过Blue/Green发布升级，保证数据可以回退。
- **肖德时注解**：微服务不是万能，先解决最需要扩展的API。对于不是瓶颈的，建议陪着业务发展了之后在扩展。把精力放在监控报警，APM等体系建设上更能产出价值。

**4.** 消息与任务队列，在Rails的项目中，都是由Sidekiq承载的。现在我想要区分消息队列与任务队列。数量级大的，不需要确保消息达到率的，可以走Kafka。数量可以接受，必须确保完成的任务，走sidekiq。但是，与API的情况类似，sidekiq与rails的ActiveRecord也有强相关。（在task里，只有一个order_id，在sidekiq的worker实际执行时，再去数据库里读取），使得独立拆分worker，也比较麻烦。目前的想法是将任务信息完整的打包在redis里，然后用go语言重写worker，独立执行。

- 基于之前的讨论，如果将models抽取出来，做成N个gem包，然后sidekiq的worker，直接引用其中的一个gem包，即可独立部署了。是否用go语言改写，可以延后在考虑。
- **肖德时注解**：对于a的做法，应该要考虑把sidekiq的服务抽象出来做成可以水平扩展的消息队列服务是可以深挖的。带来的价值会成为突破口。
- **肖德时注解**：RabbitMQ与Kafka之间的选择，主要还考虑到咱们这个项目与其他几个系统之间的通讯，之前也是采用的Kafka，所以打算收拢成一种为好。
- **肖德时注解**：对于c的选择，单一化技术栈是对的。支持。

**5.** 关于Web服务的拆分，实话实说，我都没想好要不要拆。主体Rails项目，关联到后台的MySQL，有190多个表，如果粗暴分类的话，可以分为7大类。但是，想像中要把一个数据库，拆分成七个数据库，Web Server也拆成7个Rails项目，总觉得脚下发软，心里没底。

- 目前能够想明白的思路：首先抽取models，做成N个gem包，原本的Web项目依然引用这些gem包，看起来没有变化。
- 在Web应用之前，架OpenResty+Lua脚本的服务，做分流。在一开始的时候，OpenResty后只有一个Web服务。后续拆成多个Web服务，依然对外保持一致。
- API拆分的方案，也与此类似：OpenResty做API Gateway，实现分流与限流，后面连接多个API服务。
- **肖德时注解**：微服务是为了解决业务扩展的问题，目前从实际案例中，目前还没有遇到瓶颈。从实践的角度来讲，过早的优化架构也是有技术债需要偿还的。目前可以考虑解决CI/CD的问题，加快交付流程。在过程中体会和抽取需要频繁更新的模块，然后做出服务，进行扩展使用。

### 三、分析 by 王渊命

**1.** 容器平台选择

我个人的建议是如果能搞定 kubernetes 则优先考虑，否则建议选 swarm。另外这个系统涉及的所有组件，比如 数据库，分布式存储，消息队列，都打算部署到 kubernetes 里么？还是只部署自己的微服务组件？

**2.** 镜像制作

个人建议基于 alpine 构建一套内部的镜像树，然后推广使用。按照容器的发展趋势来说，容器内的操作系统其实相当于应用的依赖，主要作用是维护软件栈，而主机操作系统是用来管理硬件的，也就是说，慢慢会演化出两种操作系统。以后新的应用或者库发布，可能就只需要考虑容器内操作系统的兼容性，而不需要在主机操作系统中进行测试。不过 alpine 的一个问题是它的 libc 不是 glibc , 而是 musl-libc ，所以许多语言（c, golang）的程序发布的时候需要在 alpine 上编译才行，不能直接使用其他 linux 发行版编译出来的二进制。这个问题也有一个办法就是先在 alpine 中安装 glibc，不过这个方式不太推荐，因为 musl-libc 是一种更轻量，简单，高效的 libc 实现，可以参看[评测结果](http://www.etalabs.net/compare_libcs.html) 。

**3.** 关于 API 以及 models 的拆分，这个我觉得是这个项目的大头，是关键点。其他的都是技术上的拆分方案，基本有通用的规则或者现成的工具，而这个是涉及业务的。鉴于我对 gitlab 本身的业务逻辑不是太熟悉，所以这里没办法给出具体的方案意见，但是可以给一些通用的建议。

gitlab 这种系统是典型的企业应用，主要是面向数据库架构的，因为大多数企业应用所承载的用户规模有限，一般也没问题。但这种架构，如果要互联网化（变为 SaaS 模式），或者遇到规模比较大的企业（比如本案例中），为了应对用户规模的增长，需要伸缩其中的某些组件，要改造成微服务模式的时候，就会遇到问题。因为 RoR 本质上是一种面向数据库的架构框架，强依赖于其 ActiveRecord 的能力，所以拆解服务需要先从数据库入手。

- 首先分析当前的数据库表和 models 之间的映射关系，粗略的按服务将数据库表归类。
- 同一个服务下的数据库表之间可以保留 ActiveRecord 的关联以及映射关系。
- 跨服务的数据库表的 ActiveRecord 的关联和映射需要解除定义，也就是说不能跨服务进行数据库查询。
- 将跨服务的 models 组装的业务逻辑从 ActiveRecord 层提升到 service 层，通过微服务的远程接口进行组装。
- 以上的拆解可以先在当前的单体应用中做，进行逻辑模块拆分，等拆分模块完毕之后，再尝试拆分项目进行独立部署。

**4.** 关于 API 网关的选择，一个是考虑 API 网关如何和服务注册中心交互，以实现动态分流，第二个是要考虑内部服务之间的通讯方式，也是通过 API 网关，还是通过 smart client 方式客户端路由，还是通过 kubernetes 这样的框架提供的 service vip 的方式。

**5.** 关于 web 的拆分，这个和 API 类似。不过这个关键点是要将 web 和后端的依赖关系完全 API 化，也就是说 web 只依赖后端的 API，而不能直接依赖数据库，或者 service。至于这样拆解后， web 是否需要再进一步拆分成更细的 service，这个可以看项目演进情况。

**6.** 消息和任务队列 这个本质上是应用的一种异步 API 的需求，量级不大的情况下，大多数应用都直接基于语言和框架内置的异步机制实现，但一旦规模上来就需要引入队列服务来实现了。至于是选择 RabbitMQ 或者 Kafka，我觉得本案例中选择一种就好，不建议引入多种队列。消息队列中的消息是 order_id 呢还是完整的消息，这个各有优劣。如果是 id 的话，worker 拿到 id 后，通过 task 服务的远程接口去查询 task 的详情也可以。

**7.** 微服务化会遇到的几个问题：

- API 输出模型的选择，胖模型还是瘦模型？比如 commit 对象中有作者信息，作者只是一个 id 呢，还是包含作者的详细信息？这个也包含异步消息中的序列化模式。瘦模型适合内部服务间互相依赖的情况，胖模型适合最终对外输出。
- 服务间的依赖关系如何界定，如何避免循环依赖？是否需要一个聚合层服务。如果服务之间都可以互相调用，很容易出现循环依赖。比如 commit 服务中由于需要输出作者信息，所以依赖 user 服务，而 user 服务输出用户信息的时候，希望也输出该用户最新的几条 commit 信息。如果出现线上的循环依赖可能会导致内网挂掉，相当于自己对自己发起了 DDOS。这种情况下，为了清晰的依赖关系，可以规定 service 之间不互相依赖，在外面再加一层聚合服务，专门做聚合输出。

**8.** 综述：

面向数据库架构的应用，要重构成微服务，其实和重写差不多。我觉得这也是为什么 twitter 选择了直接放弃 RoR，用 java 重写（虽然这里面也有语言性能的原因，但我觉得语言的性能不是关键原因，关键是是否能支撑伸缩，如果放弃 Rails，用 ruby 重写理论上也可以搞定，但没有了 rails，选择 ruby 的理由更少了）。

所以这里也建议一种方案，就是用其他语言重写。最外层的 web 和 API 聚合层可以沿用当前的 ruby，后面的微服务 API service 全部重写，逐渐替换，理论上成本并不会比基于当前的架构重构成本更高。

### 四、分析 by 肖德时

**1.** GitLab因为业务流量增加，迫切需要一套水平扩展的解决方案。这是计划微服务改造探索的起因。在负载与存储量不断上升之后，第一步需要做的事情并不是着手设计一套优雅的可扩展的架构，反而最迫切的事情是容量规划，通过监控，日志系统迅速了解当前资源可以支撑的规模到底是什么样的容量规模。预期的服务能力是什么样子，按照现有的能力扩展，到底需要多少资源才能满足。这些基础数据的整理，对日后扩展提供了一个基准线。从研发投入产出比来讲，没有对业务做出贡献的架构改造都是不充分合理的，需要注意实施步骤。

具体问题要解决，当流量上来后，通过多个rails web来分流未尝不可，但是底下的存储有状态，打破了水平扩展的规则。存储一般有容量限制，要想解决存储问题，Sharding文件还是启用分布式文件系统都是具体的解决办法。

**2.** 梳理迫切需要解决问题清单，尝试梳理如下：

- DevOps最佳实践
- 更新微服务架构设计，支持服务的水平扩展
- 资源调度，节省成本

**3.** 启用容器和编排系统的目的，就是建立企业自己的PaaS平台。从chef+ansible到PaaS平台，原来都是需要专业厂商提供，现在通过开源的框架，小的研发团队就可以快速建立起来标准、可靠的应用发布平台。Kubernetes，Swarm，Mesos都是很优秀的开源方案，选择一款开源方案作为业务支撑都可以。

**4.** 解决核心问题，业务改造。

- rails web和api是绑定的。内部model是复用。单体模式。
  - 抽离rails web，去掉db，在service层直接调用API获取资源。
  - 重写web，使用ag2、react等前端框架取代rails web。
  - rails api按照业务分离，方便局部API可以水平扩展。通过容器部署，可以动态解决资源负载的分配。
- 异步任务使用的消息队列采用一款就可以。深入积累知识作为支持储备。rabbitmq和Kafka都没有问题。为了服务水平化，可以考虑建立一套独立的消息队列cluster。

**5.** 因为rails web和db解耦之后，rails web变成了client端访问后端。后端API变成微服务之后的服务质量保障可以通过引入API 网关来解决。

**6.** Rails就是为单体而生，上微服务意义不大。把单体的rails web通过容器封装水平扩展部署。只有当业务量达到一个非常巨大的请求量时才考虑分拆。

- 强化监控报警和应用层[Tracing](http://opentracing.io/) 是团队能力提升最关键的工具。
- ELK的服务化改造也要重点关注。

**7.** 综述：

- 微服务架构是云计算发展过程中产生的一种推荐架构，是为了分布式部署而生。系统架构改造的前提是了解当前容量和服务能力的水平，然后再做架构设计和扩展。我们很多时候最大的问题是没有设立基准线，等改造完成后，结果是否满意，并不能准确的回答。大量技术的资源投入，产出比不是想出来的，是通过数据展示来验证的。
- 业务抽离可以从web层开始，而不是API层。从外往里面一层一层的拆解应该是最容易的。对外API和对内API可以分离的很清楚。一旦完成分离，通过API网关汇聚内部API服务，通过统一的指标来规约API服务的质量。
- 启用微服务之后，管理复杂度提升，相关的监控报警日志系统比原来更为重要。给自己建立一套好用的监控服务工具比业务改造的任务还要重要。
- ​